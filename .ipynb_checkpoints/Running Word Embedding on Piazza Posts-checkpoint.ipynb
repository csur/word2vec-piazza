{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embedding (very bad explanation follows) is translating each word in a corpus into a D dimension vector. These vectors are supposed to preserve semantic properties and are commonly used in NLP. For example, in a large corpus of words (say the google news corpus), the vector point for man $v_{king}$ should be near the vector point for $v_{queen}$. Furthermore, meaning such as \"king is to man as queen is to woman\" is preserved via $v_{king} - v_{queen} \\approx v_{man} - v_{queen}$. These embeddings are generated via neural networks and a common tool for doing this is Word2Vec produced by [Mikolov et al](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf). A fast implementation of Word2Vec is available in gensim.\n",
    "\n",
    "For more reading, read [the wiki](https://www.wikiwand.com/en/Word2vec) or the paper itself. For interesting experiments, read these:\n",
    "\n",
    "- [Running Word2Vec on Instagram Emojis](http://instagram-engineering.tumblr.com/post/117889701472/emojineering-part-1-machine-learning-for-emoji)\n",
    "- [Word2Vec on English Wiki](http://textminingonline.com/training-word2vec-model-on-english-wikipedia-by-gensim)\n",
    "- And, shamelessly, my favorite because I wrote it [Sentiment Analysis using Word2Vec](https://github.com/linanqiu/word2vec-sentiments)\n",
    "\n",
    "Won't it be fun to run this on Piazza posts? I'm a TA for data structures this semester (for Prof Blaer and Prof Bauer) and we have a Piazza containing 800 posts (ripe with answers, followups, etc). Speaking of that, we have an average response time of 5min. Find me another team of TAs that can beat this hurhhurh."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Corpus\n",
    "\n",
    "Turns out Piazza does not have an official API. However, there is an unofficial API. [https://github.com/hfaran/piazza-api](https://github.com/hfaran/piazza-api) That's good enough for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Email: lq2137@columbia.edu\n",
      "········\n"
     ]
    }
   ],
   "source": [
    "from piazza_api import Piazza\n",
    "piazza = Piazza()\n",
    "piazza.user_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's grab all the posts as `.json`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "course = piazza.network('ijfyurrye2g1oc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading post 0\n",
      "Downloading post 100\n",
      "Downloading post 200\n",
      "Downloading post 300\n",
      "Downloading post 400\n",
      "Downloading post 500\n",
      "Downloading post 600\n",
      "Downloading post 700\n",
      "Downloading post 800\n"
     ]
    }
   ],
   "source": [
    "posts = course.iter_all_posts()\n",
    "\n",
    "count = 0\n",
    "for post in posts:\n",
    "    if count % 100 == 0:\n",
    "        print 'Downloading post %d' % count\n",
    "    with open('%d.json' % count, 'w') as csv_file:\n",
    "        csv_file.write(json.dumps(post))\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we parse the `.json`s into a giant `.txt` by unscrambling the messy `.json` that the API provides. We also tokenize and perform some basic cleaning (mash everything to lower case). This is a very blunt tool, but should suffice for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def grab_children(post):\n",
    "    content_children = []\n",
    "    def traverse(children):\n",
    "        for child in children:\n",
    "            if 'history' in child:\n",
    "                history = child['history']\n",
    "                content_children.extend([history_item['content'] for history_item in history])\n",
    "            if 'children' in child:\n",
    "                traverse(child['children'])\n",
    "    traverse(post['children'])\n",
    "    return content_children\n",
    "\n",
    "corpus = []\n",
    "\n",
    "for glob_file in glob.glob('*.json'):\n",
    "    with open(glob_file, 'r') as json_file:\n",
    "        post = json.load(json_file)\n",
    "        content = [history['content'] for history in post['history']]\n",
    "        content.extend(grab_children(post))\n",
    "        content = [BeautifulSoup(text, 'html.parser').get_text() for text in content]\n",
    "        content = [word_tokenize(text.lower()) for text in content]\n",
    "        corpus.extend(content)\n",
    "\n",
    "with open('corpus.txt', 'w') as corpus_file:\n",
    "    for line in corpus:\n",
    "        corpus_file.write('%s\\n' % ' '.join(line).strip().encode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model\n",
    "\n",
    "Now let's train the model using `gensim`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim.models.word2vec as word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turns out `gensim` has a nice reader that iterates over a text file with one sentence a line. That's exactly what we produced in the corpus section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentences = word2vec.LineSentence('corpus.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train the model on this corpus. These are some hyperparameters that I found to be good. I shan't explain them too much, but if you want a little more detail we are essentially using the Skip-Gram with Negative Sampling portion of Word2Vec over 100 iterations and a Skip-Gram window of 15. We also discard all words that occur few than 5 times.\n",
    "\n",
    "This should take no more than a minute since the corpus is tiny (which theoretically should give us crappy results but let's see.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = word2vec.Word2Vec(sentences, min_count=5, workers=8, iter=100, window=15, size=300, negative=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "Now let's look at some results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `most_similar` method takes in a word, converts it to its vector representation, and finds the other words that are closest to it by cosine distance. These similar words should have been used in a similar context with the original word. Let's find some interesting results.\n",
    "\n",
    "First, let's do a sanity check using the word `homework`. Turns out `homework` is indeed associated with words we'd expect to be associated with `homework`: `solutions`, `grade`, `email`, and even `latex`. That's good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'solutions', 0.3142701983451843),\n",
       " (u'grade', 0.3114492893218994),\n",
       " (u'email', 0.3077431321144104),\n",
       " (u'inserted', 0.3056461215019226),\n",
       " (u'description', 0.2953464388847351),\n",
       " (u'programming', 0.2912822961807251),\n",
       " (u'latex', 0.2862958312034607),\n",
       " (u'theorem', 0.28351300954818726),\n",
       " (u'signature', 0.2821905314922333),\n",
       " (u'posts', 0.28168296813964844)]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('homework')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for something more advanced. Let's try the word `heap`. Turns out we have words that are pretty related to the `heap` concept:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'build', 0.30242919921875),\n",
       " (u'percolatedown', 0.2909688353538513),\n",
       " (u'one-by-one', 0.28867167234420776),\n",
       " (u'deletion', 0.2867096960544586),\n",
       " (u'k', 0.28530699014663696),\n",
       " (u'discussed', 0.2748313546180725),\n",
       " (u'linear', 0.26605361700057983),\n",
       " (u'increasing', 0.2620879113674164),\n",
       " (u'quicksort', 0.25420695543289185),\n",
       " (u'instructions', 0.2464582324028015)]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('heap')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the most awesome result ever. What concepts are associated with `good`? Turns out I'm one of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'great', 0.35192880034446716),\n",
       " (u'very', 0.3159177303314209),\n",
       " (u'almost', 0.30515187978744507),\n",
       " (u'concepts', 0.3004434108734131),\n",
       " (u'linan', 0.2995752990245819),\n",
       " (u'too', 0.2964840829372406),\n",
       " (u'basic', 0.2916448712348938),\n",
       " (u'fit', 0.28502458333969116),\n",
       " (u'useful', 0.2824944257736206),\n",
       " (u'answered', 0.28173112869262695)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('good')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![doge](https://cdn.thinglink.me/api/image/727110550026190849/1240/10/scaletowidth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what's similar to the profs. Turns out Prof Blaer's love for `ocaml` is well noted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'professor', 0.7219115495681763),\n",
       " (u'prof.', 0.6643839478492737),\n",
       " (u'bauer', 0.4823228120803833),\n",
       " (u'today', 0.44543683528900146),\n",
       " (u'went', 0.4129117727279663),\n",
       " (u'pm', 0.4097597897052765),\n",
       " (u'session', 0.40625864267349243),\n",
       " (u'mentioned', 0.4010222554206848),\n",
       " (u'ocaml', 0.3884058892726898),\n",
       " (u'tonight', 0.38226139545440674)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('blaer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'professor', 0.6429691314697266),\n",
       " (u'slides', 0.5277301669120789),\n",
       " (u'prof.', 0.5086715221405029),\n",
       " (u'lecture', 0.4928019642829895),\n",
       " (u'blaer', 0.4823228120803833),\n",
       " (u'sign', 0.4362599849700928),\n",
       " (u'pm', 0.3718754053115845),\n",
       " (u'went', 0.3549380302429199),\n",
       " (u'tonight', 0.326946496963501),\n",
       " (u'perform', 0.3159770965576172)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('bauer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sasha loves grades, which is unsurprising."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'email', 0.5593311786651611),\n",
       " (u'ta', 0.4426960051059723),\n",
       " (u'monday', 0.4238441586494446),\n",
       " (u'grade', 0.42380213737487793),\n",
       " (u'talk', 0.40258437395095825),\n",
       " (u'grading', 0.3625785708427429),\n",
       " (u'mentioned', 0.3540249466896057),\n",
       " (u'approaches', 0.3509276807308197),\n",
       " (u'cs', 0.3439938724040985),\n",
       " (u'hope', 0.3431258201599121)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('sasha')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'graph'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(\"bauer blaer graph\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And of course it differentiates between a TA and a professor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'linan'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(\"bauer blaer linan\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ISN'T THIS FUCKING AWESOME."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
